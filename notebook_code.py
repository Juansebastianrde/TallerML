# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1agNN7Wz3C6H-lfW6_UwEllFVgfmZX8Fx

# Base de datos
"""

# Cargar librerías
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from scipy.stats import spearmanr, stats
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.decomposition import PCA
import prince

# Subir el token
from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d ashishsahani/hospital-admissions-data

!unzip hospital-admissions-data.zip

"""## 1 Cargar base de datos"""

bd = pd.read_csv('HDHI Admission data.csv')
bd.head()

bd.info()

bd.drop('BNP', axis=1, inplace=True)

"""## Descripción de la base de datos

Este conjunto de datos corresponde a los registros de 14.845 admisiones hospitalarias (12.238 pacientes, incluyendo 1.921 con múltiples ingresos) recogidos durante un período de dos años (1 de abril de 2017 a 31 de marzo de 2019) en el Hero DMC Heart Institute, unidad del Dayanand Medical College and Hospital en Ludhiana, Punjab, India.

La información incluye:

Datos demográficos: edad, género y procedencia (rural o urbana).

Detalles de admisión: tipo de admisión (emergencia u OPD), fechas de ingreso y alta, duración total de la estancia y duración en unidad de cuidados intensivos (columna objetivo en este proyecto).

Antecedentes médicos: tabaquismo, consumo de alcohol, diabetes mellitus (DM), hipertensión (HTN), enfermedad arterial coronaria (CAD), cardiomiopatía previa (CMP), y enfermedad renal crónica (CKD).

Parámetros de laboratorio: hemoglobina (HB), conteo total de leucocitos (TLC), plaquetas, glucosa, urea, creatinina, péptido natriurético cerebral (BNP), enzimas cardíacas elevadas (RCE) y fracción de eyección (EF).

Condiciones clínicas y comorbilidades: más de 28 variables como insuficiencia cardíaca, infarto con elevación del ST (STEMI), embolia pulmonar, shock, infecciones respiratorias, entre otras.

Resultado hospitalario: estado al alta (alta médica o fallecimiento).

## 2. Tratamiento de la base de datos
"""

## Eliminar variables innevesarias

df = bd.drop(['SNO', 'MRD No.'], axis=1)
df.drop('month year', axis=1, inplace=True)

## Transformar las variables de fecha a formate datetime
df['D.O.A'] = pd.to_datetime(df['D.O.A'], format='%m/%d/%Y', errors='coerce')
df['D.O.D'] = pd.to_datetime(df['D.O.D'], format='%m/%d/%Y', errors='coerce')

# Tratamiento de aquellas variables que son numéricas pero están como categóricas

cols_to_clean = ['HB', 'TLC', 'PLATELETS', 'GLUCOSE', 'UREA', 'CREATININE', 'EF', 'CHEST INFECTION']

for col in cols_to_clean:
    df[col] = (
        df[col]
        .astype(str)                      # aseguramos que todo sea string
        .str.strip()                       # quitamos espacios
        .replace(['EMPTY', 'nan', 'NaN', 'None', ''], np.nan)  # reemplazamos valores no válidos
        .str.replace(r'[<>]', '', regex=True)  # quitamos > y <
        .str.replace(',', '.', regex=False)    # cambiamos coma decimal a punto
    )

# Convierte las variables anteriores a numéricas
for col in cols_to_clean:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# Transforma las variables categóricas a dummies

df['GENDER'] = df['GENDER'].map({'M': 1, 'F': 0})
df['RURAL'] = df['RURAL'].map({'R': 1, 'U': 0})
df['TYPE OF ADMISSION-EMERGENCY/OPD'] = df['TYPE OF ADMISSION-EMERGENCY/OPD'].map({'E': 1, 'O': 0})
df = pd.get_dummies(df, columns=['OUTCOME'], drop_first=False)

# Convierte cualquier columna booleana a int (0 y 1)

bool_cols = df.select_dtypes(include=bool).columns
df[bool_cols] = df[bool_cols].astype(int)

#transformar chest infection a numerica

df['CHEST INFECTION'] = df['CHEST INFECTION'].fillna(0).astype(np.int64)

df.columns = df.columns.str.strip()
list(df.columns)

"""### 2.1 Separación en variables categóricas y variables numéricas"""

# Separar categóricas y numéricas
cat_features = binary_cats = [
    'GENDER', 'RURAL', 'TYPE OF ADMISSION-EMERGENCY/OPD',
    'OUTCOME_DAMA', 'OUTCOME_DISCHARGE', 'OUTCOME_EXPIRY',
    'SMOKING', 'ALCOHOL', 'DM', 'HTN', 'CAD', 'PRIOR CMP', 'CKD',
    'RAISED CARDIAC ENZYMES', 'SEVERE ANAEMIA', 'ANAEMIA', 'STABLE ANGINA',
    'ACS', 'STEMI', 'ATYPICAL CHEST PAIN', 'HEART FAILURE', 'HFREF', 'HFNEF',
    'VALVULAR', 'CHB', 'SSS', 'AKI', 'CVA INFRACT', 'CVA BLEED', 'AF', 'VT', 'PSVT',
    'CONGENITAL', 'UTI', 'NEURO CARDIOGENIC SYNCOPE', 'ORTHOSTATIC',
    'INFECTIVE ENDOCARDITIS', 'DVT', 'CARDIOGENIC SHOCK', 'SHOCK',
    'PULMONARY EMBOLISM'
]
num_features = [col for col in df.columns if col not in cat_features and col not in ['D.O.A', 'D.O.D']]

num_features

df_numericas = df[num_features]
df_numericas.head()

df_categoricas = df[cat_features]
df_categoricas.head()

"""## 3. Dividir conjunto de entrenamiento y prueba

La variable elegida como objetivo es de tipo numérico continuo y representa el número de días, o fracción de días, que un paciente permanecerá en el hospital. Su predicción tiene un alto valor clínico y operativo, ya que permite planificar con mayor precisión los recursos, la disponibilidad de camas y la asignación de personal. Además, esta duración está influenciada por múltiples factores presentes en el conjunto de datos, como diagnósticos, comorbilidades y resultados de laboratorio.
"""

# Separar variables y objetivo
X = df[num_features + cat_features]  # variables
y = df['DURATION OF STAY']  # objetivo

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""### 2.2 Preprocesamiento"""

# Transformador para numéricas
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="mean")),
    ("scaler", StandardScaler())
])

# Transformador para categóricas
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent"))
])

# Combinamos en un ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, num_features),
        ("cat", categorical_transformer, cat_features)
    ]
)

# Aplicamos el preprocesamiento
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

"""## 4. Selección de características

### 1. Selección por filtrado

#### Correlación de Spearman
"""

# 1. Filtrar solo columnas numéricas continuas de tu lista df_numericas
X_train_num = X_train[num_features].copy()

# 2. Unir con y_train
train_num_with_target = X_train_num.copy()
train_num_with_target["DURATION OF STAY"] = y_train

# 3. Calcular Spearman solo para numéricas continuas
correlaciones = train_num_with_target.corr(method='spearman')["DURATION OF STAY"]

# 4. Ordenar de mayor a menor por valor absoluto
correlaciones_ordenadas = correlaciones.reindex(
    correlaciones.abs().sort_values(ascending=False).index
)

# 5. Imprimir todas
print("=== Correlaciones de Spearman (solo numéricas continuas) ===")
for col, valor in correlaciones_ordenadas.items():
    print(f"{col}: {valor:.4f}")

# 6. Calcular porcentaje y acumulado
df_corr = correlaciones_ordenadas.drop("DURATION OF STAY").to_frame(name="correlation")
df_corr["abs_corr"] = df_corr["correlation"].abs()
df_corr["percentage"] = df_corr["abs_corr"] / df_corr["abs_corr"].sum() * 100
df_corr["cum_percentage"] = df_corr["percentage"].cumsum()


# 7. Filtrar por porcentaje acumulado
umbral_acumulado = 90
numericas_significativas = df_corr[df_corr["cum_percentage"] <= umbral_acumulado].index.tolist()

plt.figure(figsize=(10, 6))
plt.bar(df_corr.index, df_corr["percentage"], label="Porcentaje individual")
plt.plot(df_corr.index, df_corr["cum_percentage"], color="red", marker="o", label="Porcentaje acumulado")
plt.axhline(umbral_acumulado, color="green", linestyle="--", label=f"Umbral {umbral_acumulado}%")
plt.xticks(rotation=90)
plt.ylabel("Porcentaje (%)")
plt.title("Importancia por Spearman y acumulado")
plt.legend()
plt.tight_layout()
plt.show()

print("\n✅ Variables numéricas seleccionadas (por 90% acumulado):")
print(numericas_significativas)

"""Teniendo en cuenta la correlación de las variables con la variable objetivo, se seleccionan las siguientes características: duration of intensive unit stay, UREA, TLC, CREATININE, HB, EF.

#### ANOVA
"""

X_train_cat = X_train[cat_features].copy()

significativas = []

for col in X_train_cat:
    grupos = [
        df[df[col] == categoria]['DURATION OF STAY'].dropna()
        for categoria in df[col].unique()
    ]
    f_stat, p_val = stats.f_oneway(*grupos)

    # Verificamos si es estadísticamente significativa
    if p_val < 0.05:
        significativas.append(col)

    print(f"{col}: F={f_stat:.3f}, p-value={p_val:.4f}")

print("\n✅ Variables categóricas significativas (p < 0.05):")
print(significativas)

"""### 2 Selección automática

SelectKBest
"""

var_seleccionadas= numericas_significativas + significativas

# Filtrar el preprocesado, no el crudo
X_train_filtrado = pd.DataFrame(
    X_train_processed,  # array preprocesado
    columns=num_features + cat_features  # columnas después del preprocesador
)[var_seleccionadas]

X_test_filtrado = pd.DataFrame(
    X_test_processed,
    columns=num_features + cat_features
)[var_seleccionadas]

# Ahora sí, aplicar SelectKBest
from sklearn.feature_selection import SelectKBest, f_regression

selector = SelectKBest(score_func=f_regression, k='all')
selector.fit(X_train_filtrado, y_train)

scores = selector.scores_
ranking = sorted(zip(var_seleccionadas, scores), key=lambda x: x[1], reverse=True)

df_scores = pd.DataFrame(ranking, columns=["Variable", "Score"])
df_scores["Perc"] = (df_scores["Score"] / df_scores["Score"].sum()) * 100
df_scores["CumPerc"] = df_scores["Perc"].cumsum()

# Mostrar
print(df_scores)

fig, ax1 = plt.subplots(figsize=(10,6))

color = "skyblue"
ax1.bar(df_scores["Variable"], df_scores["Score"], color=color)
ax1.set_xlabel("Variables")
ax1.set_ylabel("Score F-test", color=color)
ax1.tick_params(axis="y", labelcolor=color)
ax1.set_xticklabels(df_scores["Variable"], rotation=45, ha="right")

ax2 = ax1.twinx()
color = "crimson"
ax2.plot(df_scores["Variable"], df_scores["CumPerc"], color=color, marker="o")
ax2.set_ylabel("Cumulative %", color=color)
ax2.tick_params(axis="y", labelcolor=color)
ax2.set_ylim(0, 110)

# 5. Línea de referencia al o 90%
ax2.axhline(y=90, color="gray", linestyle="--", linewidth=1)
ax2.text(len(df_scores)-1, 82, "90% threshold", color="gray")

plt.title("SelectKBest - Importance & Cumulative %")
plt.tight_layout()
plt.show()

# Filtrar hasta el 90%
vars_90 = df_scores[df_scores["CumPerc"] <= 90]['Variable'].tolist()

print(f"Variables que acumulan el 90%: {vars_90}")
print(f"Cantidad: {len(vars_90)}")

"""RFE"""

# Modelo base para RFE
modelo_rfe = LinearRegression()

# RFE para seleccionar, por ejemplo, las 10 mejores
selector_rfe = RFE(modelo_rfe, n_features_to_select=10)
selector_rfe.fit(X_train_filtrado, y_train)

# Variables seleccionadas
selected_features_rfe = X_train_filtrado.columns[selector_rfe.support_]
print("\n✅ Variables seleccionadas por RFE:")
print(selected_features_rfe.tolist())

"""### Random Forest"""

# Entrenar modelo
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train_filtrado, y_train)

# Importancia de características
importances = rf_model.feature_importances_
importances_df = pd.DataFrame({
    'Variable': X_train_filtrado.columns,
    'Importancia': importances
}).sort_values(by='Importancia', ascending=False)

# Calcular porcentaje acumulado
importances_df['Importancia_Acum'] = importances_df['Importancia'].cumsum()

# Seleccionar variables hasta el 90%
selected_vars = importances_df[importances_df['Importancia_Acum'] <= 0.90]['Variable'].tolist()

print("\n🌲 Ranking de importancia con RandomForest:")
print(importances_df)
print("\n✅ Variables seleccionadas hasta el 90% de importancia acumulada:")
print(selected_vars)

# Filtrar dataset
X_train_sel = X_train_filtrado[selected_vars]
X_test_sel = X_test_filtrado[selected_vars]

# Gráfica de importancias y acumulado
plt.figure(figsize=(10, 6))

# Barras de importancia
plt.bar(importances_df['Variable'], importances_df['Importancia'], alpha=0.7, label='Importancia individual')

# Línea de importancia acumulada
plt.plot(importances_df['Variable'], importances_df['Importancia_Acum'], marker='o', color='red', label='Importancia acumulada')

# Línea de referencia del 90%
plt.axhline(0.90, color='green', linestyle='--', label='90% acumulado')

# Formato del gráfico
plt.xticks(rotation=90)
plt.ylabel('Importancia')
plt.title('Ranking de importancia de variables - RandomForest')
plt.legend()
plt.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

# Convertimos a conjuntos y sacamos la intersección
comunes = list(set(vars_90) & set(selected_features_rfe) )

print("Variables en común:", comunes)

# Filtrar DataFrame solo con esas columnas
df_filtrado = df[comunes]
print("\nDataFrame filtrado:")
df_filtrado.head()

"""En primera instancia, se aplicaron métodos de filtrado utilizando la medida de correlación de Spearman y la prueba ANOVA, seleccionando en cada caso las variables con mayor relevancia estadística. Posteriormente, con el conjunto reducido obtenido de esta etapa, se implementaron tres métodos de selección de características: SelectKBest, RFE y Random Forest. Finalmente, se identificaron las variables comunes entre SelectKBest y RFE, las cuales fueron seleccionadas para conformar un nuevo dataframe con menor dimensionalidad, optimizando así la eficiencia del modelo sin comprometer su capacidad predictiva.

### PCA Y MCA
"""

# 1. Obtener índices de columnas numéricas en el dataset crudo
num_indices = [i for i, col in enumerate(X_train.columns) if col in num_features]

# 2. Filtrar las columnas procesadas usando esos índices
X_train_numericas = pd.DataFrame(
    X_train_processed[:, num_indices],
    columns=num_features
)

X_test_numericas = pd.DataFrame(
    X_test_processed[:, num_indices],
    columns=num_features
)

# PCA (elige 70% de var. explicada automáticamente)
pca = PCA(n_components=0.70, random_state=42)
Xn_train_pca = pca.fit_transform(X_train_numericas)
Xn_test_pca  = pca.transform(X_test_numericas)

pca_names = [f'PCA{i+1}' for i in range(Xn_train_pca.shape[1])]
Xn_train_pca = pd.DataFrame(Xn_train_pca, columns=pca_names, index=X_train.index)
Xn_test_pca  = pd.DataFrame(Xn_test_pca,  columns=pca_names, index=X_test.index)

print(f'PCA: {len(pca_names)} componentes, var. explicada acumulada = {pca.explained_variance_ratio_.sum():.3f}')

# Varianza explicada por cada componente
var_exp = pca.explained_variance_ratio_
cum_var_exp = np.cumsum(var_exp)

plt.figure(figsize=(8,5))

# Barras de varianza explicada individual
plt.bar(range(1, len(var_exp)+1), var_exp, alpha=0.6, label='Varianza explicada por componente')

# Línea de varianza acumulada
plt.step(range(1, len(cum_var_exp)+1), cum_var_exp, where='mid', color='red', label='Varianza acumulada')

# Línea horizontal del 70% (opcional, ya que usaste 0.70)
plt.axhline(y=0.70, color='green', linestyle='--', label='70%')

plt.xlabel('Componentes principales')
plt.ylabel('Proporción de varianza explicada')
plt.title('PCA - Varianza explicada y acumulada')
plt.xticks(range(1, len(var_exp)+1))
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# 1. Obtener índices de las columnas categóricas en el X_train original
cat_indices = [i for i, col in enumerate(X_train.columns) if col in cat_features]

# 2. Filtrar las columnas categóricas procesadas
X_train_categoricas = pd.DataFrame(
    X_train_processed[:, cat_indices],
    columns=cat_features
)

X_test_categoricas = pd.DataFrame(
    X_test_processed[:, cat_indices],
    columns=cat_features
)

# 3. MCA con prince
import prince

mca = prince.MCA(
    n_components=5,
    n_iter=5,
    random_state=42
)
mca.fit(X_train_categoricas)

Xc_train_mca = mca.transform(X_train_categoricas)
Xc_test_mca  = mca.transform(X_test_categoricas)
Xc_train_mca.index = X_train.index
Xc_test_mca.index  = X_test.index

# 4. Renombrar componentes
mca_names = [f'MCA{i+1}' for i in range(Xc_train_mca.shape[1])]
Xc_train_mca.columns = mca_names
Xc_test_mca.columns  = mca_names

# 5. Porcentaje de varianza (inercia)
ev = mca.eigenvalues_summary
print('MCA inercia por eje:', ev['% of variance'].values)

# Convertir a numérico, quitando símbolos si es necesario
ev['% of variance'] = ev['% of variance'].replace('%', '', regex=True)  # elimina el símbolo %
ev['% of variance'] = ev['% of variance'].str.replace(',', '.', regex=False)  # cambia coma por punto
ev['% of variance'] = pd.to_numeric(ev['% of variance'], errors='coerce')

# Ahora sí, en proporción
var_exp_mca = ev['% of variance'].values / 100
cum_var_exp_mca = np.cumsum(var_exp_mca)

plt.figure(figsize=(8,5))
plt.plot(range(1, len(var_exp_mca) + 1), cum_var_exp_mca, marker='o')
plt.xlabel('Dimensiones')
plt.ylabel('Varianza Acumulada')
plt.grid(True)
plt.show()

# Asegurar que los valores sean numéricos
ev['% of variance'] = pd.to_numeric(ev['% of variance'], errors='coerce')

# Varianza explicada en proporción y acumulada
var_exp_mca = ev['% of variance'].values / 100
cum_var_exp_mca = np.cumsum(var_exp_mca)
componentes = np.arange(1, len(var_exp_mca) + 1)

# Gráfica
plt.figure(figsize=(8, 5))

# Barras: varianza explicada individual
plt.bar(componentes, var_exp_mca, alpha=0.7, label='Varianza explicada')

# Línea: varianza acumulada
plt.plot(componentes, cum_var_exp_mca, marker='o', color='red', label='Varianza acumulada')

# Formato
plt.xticks(componentes)
plt.xlabel('Componentes MCA')
plt.ylabel('Proporción de varianza')
plt.title('Scree plot - MCA')
plt.legend()
plt.grid(alpha=0.3)

plt.show()

X_train_reduced = pd.concat([Xn_train_pca, Xc_train_mca], axis=1)
X_test_reduced  = pd.concat([Xn_test_pca,  Xc_test_mca],  axis=1)

print('Shape train reducido:', X_train_reduced.shape)
print('Shape test  reducido:', X_test_reduced.shape)

X_train_reduced = pd.concat(
    [Xn_train_pca.reset_index(drop=True),
     Xc_train_mca.reset_index(drop=True)],
    axis=1
)
X_train_reduced.head(10)

"""

Posteriormente, se realizaron análisis de componentes principales (PCA) y de correspondencias múltiples (MCA) con el fin de identificar patrones y reducir la dimensionalidad de los datos. El MCA presentó una inercia por eje de 8.21%, 5.03%, 4.94%, 4.05% y 3.64%, mientras que el PCA retuvo 6 componentes que explicaron de forma acumulada el 74.8% de la varianza total. Los resultados obtenidos de ambos análisis se integraron en un nuevo *dataframe* para su posterior uso en la modelación.
"""