# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1agNN7Wz3C6H-lfW6_UwEllFVgfmZX8Fx

# Base de datos
"""

# Cargar librer√≠as
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from scipy.stats import spearmanr, stats
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.decomposition import PCA
import prince

# Subir el token
from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d ashishsahani/hospital-admissions-data

!unzip hospital-admissions-data.zip

"""## 1 Cargar base de datos"""

bd = pd.read_csv('HDHI Admission data.csv')
bd.head()

bd.info()

bd.drop('BNP', axis=1, inplace=True)

"""## Descripci√≥n de la base de datos

Este conjunto de datos corresponde a los registros de 14.845 admisiones hospitalarias (12.238 pacientes, incluyendo 1.921 con m√∫ltiples ingresos) recogidos durante un per√≠odo de dos a√±os (1 de abril de 2017 a 31 de marzo de 2019) en el Hero DMC Heart Institute, unidad del Dayanand Medical College and Hospital en Ludhiana, Punjab, India.

La informaci√≥n incluye:

Datos demogr√°ficos: edad, g√©nero y procedencia (rural o urbana).

Detalles de admisi√≥n: tipo de admisi√≥n (emergencia u OPD), fechas de ingreso y alta, duraci√≥n total de la estancia y duraci√≥n en unidad de cuidados intensivos (columna objetivo en este proyecto).

Antecedentes m√©dicos: tabaquismo, consumo de alcohol, diabetes mellitus (DM), hipertensi√≥n (HTN), enfermedad arterial coronaria (CAD), cardiomiopat√≠a previa (CMP), y enfermedad renal cr√≥nica (CKD).

Par√°metros de laboratorio: hemoglobina (HB), conteo total de leucocitos (TLC), plaquetas, glucosa, urea, creatinina, p√©ptido natriur√©tico cerebral (BNP), enzimas card√≠acas elevadas (RCE) y fracci√≥n de eyecci√≥n (EF).

Condiciones cl√≠nicas y comorbilidades: m√°s de 28 variables como insuficiencia card√≠aca, infarto con elevaci√≥n del ST (STEMI), embolia pulmonar, shock, infecciones respiratorias, entre otras.

Resultado hospitalario: estado al alta (alta m√©dica o fallecimiento).

## 2. Tratamiento de la base de datos
"""

## Eliminar variables innevesarias

df = bd.drop(['SNO', 'MRD No.'], axis=1)
df.drop('month year', axis=1, inplace=True)

## Transformar las variables de fecha a formate datetime
df['D.O.A'] = pd.to_datetime(df['D.O.A'], format='%m/%d/%Y', errors='coerce')
df['D.O.D'] = pd.to_datetime(df['D.O.D'], format='%m/%d/%Y', errors='coerce')

# Tratamiento de aquellas variables que son num√©ricas pero est√°n como categ√≥ricas

cols_to_clean = ['HB', 'TLC', 'PLATELETS', 'GLUCOSE', 'UREA', 'CREATININE', 'EF', 'CHEST INFECTION']

for col in cols_to_clean:
    df[col] = (
        df[col]
        .astype(str)                      # aseguramos que todo sea string
        .str.strip()                       # quitamos espacios
        .replace(['EMPTY', 'nan', 'NaN', 'None', ''], np.nan)  # reemplazamos valores no v√°lidos
        .str.replace(r'[<>]', '', regex=True)  # quitamos > y <
        .str.replace(',', '.', regex=False)    # cambiamos coma decimal a punto
    )

# Convierte las variables anteriores a num√©ricas
for col in cols_to_clean:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# Transforma las variables categ√≥ricas a dummies

df['GENDER'] = df['GENDER'].map({'M': 1, 'F': 0})
df['RURAL'] = df['RURAL'].map({'R': 1, 'U': 0})
df['TYPE OF ADMISSION-EMERGENCY/OPD'] = df['TYPE OF ADMISSION-EMERGENCY/OPD'].map({'E': 1, 'O': 0})
df = pd.get_dummies(df, columns=['OUTCOME'], drop_first=False)

# Convierte cualquier columna booleana a int (0 y 1)

bool_cols = df.select_dtypes(include=bool).columns
df[bool_cols] = df[bool_cols].astype(int)

#transformar chest infection a numerica

df['CHEST INFECTION'] = df['CHEST INFECTION'].fillna(0).astype(np.int64)

df.columns = df.columns.str.strip()
list(df.columns)

"""### 2.1 Separaci√≥n en variables categ√≥ricas y variables num√©ricas"""

# Separar categ√≥ricas y num√©ricas
cat_features = binary_cats = [
    'GENDER', 'RURAL', 'TYPE OF ADMISSION-EMERGENCY/OPD',
    'OUTCOME_DAMA', 'OUTCOME_DISCHARGE', 'OUTCOME_EXPIRY',
    'SMOKING', 'ALCOHOL', 'DM', 'HTN', 'CAD', 'PRIOR CMP', 'CKD',
    'RAISED CARDIAC ENZYMES', 'SEVERE ANAEMIA', 'ANAEMIA', 'STABLE ANGINA',
    'ACS', 'STEMI', 'ATYPICAL CHEST PAIN', 'HEART FAILURE', 'HFREF', 'HFNEF',
    'VALVULAR', 'CHB', 'SSS', 'AKI', 'CVA INFRACT', 'CVA BLEED', 'AF', 'VT', 'PSVT',
    'CONGENITAL', 'UTI', 'NEURO CARDIOGENIC SYNCOPE', 'ORTHOSTATIC',
    'INFECTIVE ENDOCARDITIS', 'DVT', 'CARDIOGENIC SHOCK', 'SHOCK',
    'PULMONARY EMBOLISM'
]
num_features = [col for col in df.columns if col not in cat_features and col not in ['D.O.A', 'D.O.D']]

num_features

df_numericas = df[num_features]
df_numericas.head()

df_categoricas = df[cat_features]
df_categoricas.head()

"""## 3. Dividir conjunto de entrenamiento y prueba

La variable elegida como objetivo es de tipo num√©rico continuo y representa el n√∫mero de d√≠as, o fracci√≥n de d√≠as, que un paciente permanecer√° en el hospital. Su predicci√≥n tiene un alto valor cl√≠nico y operativo, ya que permite planificar con mayor precisi√≥n los recursos, la disponibilidad de camas y la asignaci√≥n de personal. Adem√°s, esta duraci√≥n est√° influenciada por m√∫ltiples factores presentes en el conjunto de datos, como diagn√≥sticos, comorbilidades y resultados de laboratorio.
"""

# Separar variables y objetivo
X = df[num_features + cat_features]  # variables
y = df['DURATION OF STAY']  # objetivo

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""### 2.2 Preprocesamiento"""

# Transformador para num√©ricas
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="mean")),
    ("scaler", StandardScaler())
])

# Transformador para categ√≥ricas
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent"))
])

# Combinamos en un ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, num_features),
        ("cat", categorical_transformer, cat_features)
    ]
)

# Aplicamos el preprocesamiento
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

"""## 4. Selecci√≥n de caracter√≠sticas

### 1. Selecci√≥n por filtrado

#### Correlaci√≥n de Spearman
"""

# 1. Filtrar solo columnas num√©ricas continuas de tu lista df_numericas
X_train_num = X_train[num_features].copy()

# 2. Unir con y_train
train_num_with_target = X_train_num.copy()
train_num_with_target["DURATION OF STAY"] = y_train

# 3. Calcular Spearman solo para num√©ricas continuas
correlaciones = train_num_with_target.corr(method='spearman')["DURATION OF STAY"]

# 4. Ordenar de mayor a menor por valor absoluto
correlaciones_ordenadas = correlaciones.reindex(
    correlaciones.abs().sort_values(ascending=False).index
)

# 5. Imprimir todas
print("=== Correlaciones de Spearman (solo num√©ricas continuas) ===")
for col, valor in correlaciones_ordenadas.items():
    print(f"{col}: {valor:.4f}")

# 6. Calcular porcentaje y acumulado
df_corr = correlaciones_ordenadas.drop("DURATION OF STAY").to_frame(name="correlation")
df_corr["abs_corr"] = df_corr["correlation"].abs()
df_corr["percentage"] = df_corr["abs_corr"] / df_corr["abs_corr"].sum() * 100
df_corr["cum_percentage"] = df_corr["percentage"].cumsum()


# 7. Filtrar por porcentaje acumulado
umbral_acumulado = 90
numericas_significativas = df_corr[df_corr["cum_percentage"] <= umbral_acumulado].index.tolist()

plt.figure(figsize=(10, 6))
plt.bar(df_corr.index, df_corr["percentage"], label="Porcentaje individual")
plt.plot(df_corr.index, df_corr["cum_percentage"], color="red", marker="o", label="Porcentaje acumulado")
plt.axhline(umbral_acumulado, color="green", linestyle="--", label=f"Umbral {umbral_acumulado}%")
plt.xticks(rotation=90)
plt.ylabel("Porcentaje (%)")
plt.title("Importancia por Spearman y acumulado")
plt.legend()
plt.tight_layout()
plt.show()

print("\n‚úÖ Variables num√©ricas seleccionadas (por 90% acumulado):")
print(numericas_significativas)

"""Teniendo en cuenta la correlaci√≥n de las variables con la variable objetivo, se seleccionan las siguientes caracter√≠sticas: duration of intensive unit stay, UREA, TLC, CREATININE, HB, EF.

#### ANOVA
"""

X_train_cat = X_train[cat_features].copy()

significativas = []

for col in X_train_cat:
    grupos = [
        df[df[col] == categoria]['DURATION OF STAY'].dropna()
        for categoria in df[col].unique()
    ]
    f_stat, p_val = stats.f_oneway(*grupos)

    # Verificamos si es estad√≠sticamente significativa
    if p_val < 0.05:
        significativas.append(col)

    print(f"{col}: F={f_stat:.3f}, p-value={p_val:.4f}")

print("\n‚úÖ Variables categ√≥ricas significativas (p < 0.05):")
print(significativas)

"""### 2 Selecci√≥n autom√°tica

SelectKBest
"""

var_seleccionadas= numericas_significativas + significativas

# Filtrar el preprocesado, no el crudo
X_train_filtrado = pd.DataFrame(
    X_train_processed,  # array preprocesado
    columns=num_features + cat_features  # columnas despu√©s del preprocesador
)[var_seleccionadas]

X_test_filtrado = pd.DataFrame(
    X_test_processed,
    columns=num_features + cat_features
)[var_seleccionadas]

# Ahora s√≠, aplicar SelectKBest
from sklearn.feature_selection import SelectKBest, f_regression

selector = SelectKBest(score_func=f_regression, k='all')
selector.fit(X_train_filtrado, y_train)

scores = selector.scores_
ranking = sorted(zip(var_seleccionadas, scores), key=lambda x: x[1], reverse=True)

df_scores = pd.DataFrame(ranking, columns=["Variable", "Score"])
df_scores["Perc"] = (df_scores["Score"] / df_scores["Score"].sum()) * 100
df_scores["CumPerc"] = df_scores["Perc"].cumsum()

# Mostrar
print(df_scores)

fig, ax1 = plt.subplots(figsize=(10,6))

color = "skyblue"
ax1.bar(df_scores["Variable"], df_scores["Score"], color=color)
ax1.set_xlabel("Variables")
ax1.set_ylabel("Score F-test", color=color)
ax1.tick_params(axis="y", labelcolor=color)
ax1.set_xticklabels(df_scores["Variable"], rotation=45, ha="right")

ax2 = ax1.twinx()
color = "crimson"
ax2.plot(df_scores["Variable"], df_scores["CumPerc"], color=color, marker="o")
ax2.set_ylabel("Cumulative %", color=color)
ax2.tick_params(axis="y", labelcolor=color)
ax2.set_ylim(0, 110)

# 5. L√≠nea de referencia al o 90%
ax2.axhline(y=90, color="gray", linestyle="--", linewidth=1)
ax2.text(len(df_scores)-1, 82, "90% threshold", color="gray")

plt.title("SelectKBest - Importance & Cumulative %")
plt.tight_layout()
plt.show()

# Filtrar hasta el 90%
vars_90 = df_scores[df_scores["CumPerc"] <= 90]['Variable'].tolist()

print(f"Variables que acumulan el 90%: {vars_90}")
print(f"Cantidad: {len(vars_90)}")

"""RFE"""

# Modelo base para RFE
modelo_rfe = LinearRegression()

# RFE para seleccionar, por ejemplo, las 10 mejores
selector_rfe = RFE(modelo_rfe, n_features_to_select=10)
selector_rfe.fit(X_train_filtrado, y_train)

# Variables seleccionadas
selected_features_rfe = X_train_filtrado.columns[selector_rfe.support_]
print("\n‚úÖ Variables seleccionadas por RFE:")
print(selected_features_rfe.tolist())

"""### Random Forest"""

# Entrenar modelo
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train_filtrado, y_train)

# Importancia de caracter√≠sticas
importances = rf_model.feature_importances_
importances_df = pd.DataFrame({
    'Variable': X_train_filtrado.columns,
    'Importancia': importances
}).sort_values(by='Importancia', ascending=False)

# Calcular porcentaje acumulado
importances_df['Importancia_Acum'] = importances_df['Importancia'].cumsum()

# Seleccionar variables hasta el 90%
selected_vars = importances_df[importances_df['Importancia_Acum'] <= 0.90]['Variable'].tolist()

print("\nüå≤ Ranking de importancia con RandomForest:")
print(importances_df)
print("\n‚úÖ Variables seleccionadas hasta el 90% de importancia acumulada:")
print(selected_vars)

# Filtrar dataset
X_train_sel = X_train_filtrado[selected_vars]
X_test_sel = X_test_filtrado[selected_vars]

# Gr√°fica de importancias y acumulado
plt.figure(figsize=(10, 6))

# Barras de importancia
plt.bar(importances_df['Variable'], importances_df['Importancia'], alpha=0.7, label='Importancia individual')

# L√≠nea de importancia acumulada
plt.plot(importances_df['Variable'], importances_df['Importancia_Acum'], marker='o', color='red', label='Importancia acumulada')

# L√≠nea de referencia del 90%
plt.axhline(0.90, color='green', linestyle='--', label='90% acumulado')

# Formato del gr√°fico
plt.xticks(rotation=90)
plt.ylabel('Importancia')
plt.title('Ranking de importancia de variables - RandomForest')
plt.legend()
plt.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

# Convertimos a conjuntos y sacamos la intersecci√≥n
comunes = list(set(vars_90) & set(selected_features_rfe) )

print("Variables en com√∫n:", comunes)

# Filtrar DataFrame solo con esas columnas
df_filtrado = df[comunes]
print("\nDataFrame filtrado:")
df_filtrado.head()

"""En primera instancia, se aplicaron m√©todos de filtrado utilizando la medida de correlaci√≥n de Spearman y la prueba ANOVA, seleccionando en cada caso las variables con mayor relevancia estad√≠stica. Posteriormente, con el conjunto reducido obtenido de esta etapa, se implementaron tres m√©todos de selecci√≥n de caracter√≠sticas: SelectKBest, RFE y Random Forest. Finalmente, se identificaron las variables comunes entre SelectKBest y RFE, las cuales fueron seleccionadas para conformar un nuevo dataframe con menor dimensionalidad, optimizando as√≠ la eficiencia del modelo sin comprometer su capacidad predictiva.

### PCA Y MCA
"""

# 1. Obtener √≠ndices de columnas num√©ricas en el dataset crudo
num_indices = [i for i, col in enumerate(X_train.columns) if col in num_features]

# 2. Filtrar las columnas procesadas usando esos √≠ndices
X_train_numericas = pd.DataFrame(
    X_train_processed[:, num_indices],
    columns=num_features
)

X_test_numericas = pd.DataFrame(
    X_test_processed[:, num_indices],
    columns=num_features
)

# PCA (elige 70% de var. explicada autom√°ticamente)
pca = PCA(n_components=0.70, random_state=42)
Xn_train_pca = pca.fit_transform(X_train_numericas)
Xn_test_pca  = pca.transform(X_test_numericas)

pca_names = [f'PCA{i+1}' for i in range(Xn_train_pca.shape[1])]
Xn_train_pca = pd.DataFrame(Xn_train_pca, columns=pca_names, index=X_train.index)
Xn_test_pca  = pd.DataFrame(Xn_test_pca,  columns=pca_names, index=X_test.index)

print(f'PCA: {len(pca_names)} componentes, var. explicada acumulada = {pca.explained_variance_ratio_.sum():.3f}')

# Varianza explicada por cada componente
var_exp = pca.explained_variance_ratio_
cum_var_exp = np.cumsum(var_exp)

plt.figure(figsize=(8,5))

# Barras de varianza explicada individual
plt.bar(range(1, len(var_exp)+1), var_exp, alpha=0.6, label='Varianza explicada por componente')

# L√≠nea de varianza acumulada
plt.step(range(1, len(cum_var_exp)+1), cum_var_exp, where='mid', color='red', label='Varianza acumulada')

# L√≠nea horizontal del 70% (opcional, ya que usaste 0.70)
plt.axhline(y=0.70, color='green', linestyle='--', label='70%')

plt.xlabel('Componentes principales')
plt.ylabel('Proporci√≥n de varianza explicada')
plt.title('PCA - Varianza explicada y acumulada')
plt.xticks(range(1, len(var_exp)+1))
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# 1. Obtener √≠ndices de las columnas categ√≥ricas en el X_train original
cat_indices = [i for i, col in enumerate(X_train.columns) if col in cat_features]

# 2. Filtrar las columnas categ√≥ricas procesadas
X_train_categoricas = pd.DataFrame(
    X_train_processed[:, cat_indices],
    columns=cat_features
)

X_test_categoricas = pd.DataFrame(
    X_test_processed[:, cat_indices],
    columns=cat_features
)

# 3. MCA con prince
import prince

mca = prince.MCA(
    n_components=5,
    n_iter=5,
    random_state=42
)
mca.fit(X_train_categoricas)

Xc_train_mca = mca.transform(X_train_categoricas)
Xc_test_mca  = mca.transform(X_test_categoricas)
Xc_train_mca.index = X_train.index
Xc_test_mca.index  = X_test.index

# 4. Renombrar componentes
mca_names = [f'MCA{i+1}' for i in range(Xc_train_mca.shape[1])]
Xc_train_mca.columns = mca_names
Xc_test_mca.columns  = mca_names

# 5. Porcentaje de varianza (inercia)
ev = mca.eigenvalues_summary
print('MCA inercia por eje:', ev['% of variance'].values)

# Convertir a num√©rico, quitando s√≠mbolos si es necesario
ev['% of variance'] = ev['% of variance'].replace('%', '', regex=True)  # elimina el s√≠mbolo %
ev['% of variance'] = ev['% of variance'].str.replace(',', '.', regex=False)  # cambia coma por punto
ev['% of variance'] = pd.to_numeric(ev['% of variance'], errors='coerce')

# Ahora s√≠, en proporci√≥n
var_exp_mca = ev['% of variance'].values / 100
cum_var_exp_mca = np.cumsum(var_exp_mca)

plt.figure(figsize=(8,5))
plt.plot(range(1, len(var_exp_mca) + 1), cum_var_exp_mca, marker='o')
plt.xlabel('Dimensiones')
plt.ylabel('Varianza Acumulada')
plt.grid(True)
plt.show()

# Asegurar que los valores sean num√©ricos
ev['% of variance'] = pd.to_numeric(ev['% of variance'], errors='coerce')

# Varianza explicada en proporci√≥n y acumulada
var_exp_mca = ev['% of variance'].values / 100
cum_var_exp_mca = np.cumsum(var_exp_mca)
componentes = np.arange(1, len(var_exp_mca) + 1)

# Gr√°fica
plt.figure(figsize=(8, 5))

# Barras: varianza explicada individual
plt.bar(componentes, var_exp_mca, alpha=0.7, label='Varianza explicada')

# L√≠nea: varianza acumulada
plt.plot(componentes, cum_var_exp_mca, marker='o', color='red', label='Varianza acumulada')

# Formato
plt.xticks(componentes)
plt.xlabel('Componentes MCA')
plt.ylabel('Proporci√≥n de varianza')
plt.title('Scree plot - MCA')
plt.legend()
plt.grid(alpha=0.3)

plt.show()

X_train_reduced = pd.concat([Xn_train_pca, Xc_train_mca], axis=1)
X_test_reduced  = pd.concat([Xn_test_pca,  Xc_test_mca],  axis=1)

print('Shape train reducido:', X_train_reduced.shape)
print('Shape test  reducido:', X_test_reduced.shape)

X_train_reduced = pd.concat(
    [Xn_train_pca.reset_index(drop=True),
     Xc_train_mca.reset_index(drop=True)],
    axis=1
)
X_train_reduced.head(10)

"""

Posteriormente, se realizaron an√°lisis de componentes principales (PCA) y de correspondencias m√∫ltiples (MCA) con el fin de identificar patrones y reducir la dimensionalidad de los datos. El MCA present√≥ una inercia por eje de 8.21%, 5.03%, 4.94%, 4.05% y 3.64%, mientras que el PCA retuvo 6 componentes que explicaron de forma acumulada el 74.8% de la varianza total. Los resultados obtenidos de ambos an√°lisis se integraron en un nuevo *dataframe* para su posterior uso en la modelaci√≥n.
"""